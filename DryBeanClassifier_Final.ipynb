{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    log_loss, roc_auc_score, roc_curve, precision_recall_curve,\n",
    "    cohen_kappa_score)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data, preprocessing it, and splitting into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportions: \n",
      " 0\n",
      "3    0.260562\n",
      "6    0.193699\n",
      "5    0.148880\n",
      "4    0.141624\n",
      "2    0.119765\n",
      "0    0.097079\n",
      "1    0.038391\n",
      "Name: proportion, dtype: float64\n",
      "0\n",
      "3    0.260375\n",
      "6    0.193537\n",
      "5    0.149100\n",
      "4    0.141755\n",
      "2    0.119721\n",
      "0    0.097319\n",
      "1    0.038193\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "bean_data = pd.read_csv('Dry_Bean_Dataset.csv')\n",
    "\n",
    "X=bean_data.drop(columns=['Class'])\n",
    "\n",
    "min_max = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_scaled = min_max.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled)\n",
    "X_scaled\n",
    "\n",
    "y = bean_data.Class\n",
    "# Encode labels if they are categorical\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "#confirming proportions are the same after the strat split\n",
    "print('Proportions: \\n', pd.DataFrame(y_train).value_counts(normalize=True))\n",
    "print(pd.DataFrame(y_test).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forming the individual models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN with some parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from my_dir\\tune_tabular_model\\tuner0.json\n",
      "Best Hyperparameters: {'units_layer1': 192, 'dropout_layer1': 0.30000000000000004, 'units_layer2': 64, 'dropout_layer2': 0.30000000000000004, 'learning_rate': 0.001}\n",
      "Epoch 1/50\n",
      "273/273 [==============================] - 2s 4ms/step - loss: 0.7578 - accuracy: 0.7254 - val_loss: 0.3037 - val_accuracy: 0.8930\n",
      "Epoch 2/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.3603 - accuracy: 0.8718 - val_loss: 0.2433 - val_accuracy: 0.9137\n",
      "Epoch 3/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.3008 - accuracy: 0.8962 - val_loss: 0.2304 - val_accuracy: 0.9178\n",
      "Epoch 4/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2885 - accuracy: 0.8979 - val_loss: 0.2317 - val_accuracy: 0.9164\n",
      "Epoch 5/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2744 - accuracy: 0.9055 - val_loss: 0.2250 - val_accuracy: 0.9183\n",
      "Epoch 6/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2690 - accuracy: 0.9053 - val_loss: 0.2394 - val_accuracy: 0.9187\n",
      "Epoch 7/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2574 - accuracy: 0.9083 - val_loss: 0.2147 - val_accuracy: 0.9219\n",
      "Epoch 8/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2526 - accuracy: 0.9113 - val_loss: 0.2442 - val_accuracy: 0.9137\n",
      "Epoch 9/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2527 - accuracy: 0.9110 - val_loss: 0.2179 - val_accuracy: 0.9164\n",
      "Epoch 10/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2542 - accuracy: 0.9106 - val_loss: 0.2143 - val_accuracy: 0.9201\n",
      "Epoch 11/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2431 - accuracy: 0.9154 - val_loss: 0.2117 - val_accuracy: 0.9247\n",
      "Epoch 12/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2440 - accuracy: 0.9150 - val_loss: 0.2111 - val_accuracy: 0.9316\n",
      "Epoch 13/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2411 - accuracy: 0.9161 - val_loss: 0.2131 - val_accuracy: 0.9252\n",
      "Epoch 14/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2395 - accuracy: 0.9171 - val_loss: 0.2188 - val_accuracy: 0.9252\n",
      "Epoch 15/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2381 - accuracy: 0.9164 - val_loss: 0.2013 - val_accuracy: 0.9288\n",
      "Epoch 16/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2368 - accuracy: 0.9164 - val_loss: 0.2026 - val_accuracy: 0.9311\n",
      "Epoch 17/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2374 - accuracy: 0.9169 - val_loss: 0.2054 - val_accuracy: 0.9270\n",
      "Epoch 18/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2285 - accuracy: 0.9181 - val_loss: 0.2139 - val_accuracy: 0.9206\n",
      "Epoch 19/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2321 - accuracy: 0.9185 - val_loss: 0.1979 - val_accuracy: 0.9284\n",
      "Epoch 20/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2290 - accuracy: 0.9166 - val_loss: 0.1920 - val_accuracy: 0.9307\n",
      "Epoch 21/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2305 - accuracy: 0.9184 - val_loss: 0.2227 - val_accuracy: 0.9137\n",
      "Epoch 22/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2304 - accuracy: 0.9192 - val_loss: 0.1964 - val_accuracy: 0.9311\n",
      "Epoch 23/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2193 - accuracy: 0.9199 - val_loss: 0.2042 - val_accuracy: 0.9325\n",
      "Epoch 24/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2248 - accuracy: 0.9203 - val_loss: 0.2029 - val_accuracy: 0.9270\n",
      "Epoch 25/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2215 - accuracy: 0.9218 - val_loss: 0.2012 - val_accuracy: 0.9261\n",
      "Epoch 26/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9215 - val_loss: 0.1945 - val_accuracy: 0.9288\n",
      "Epoch 27/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2202 - accuracy: 0.9222 - val_loss: 0.1936 - val_accuracy: 0.9279\n",
      "Epoch 28/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2220 - accuracy: 0.9191 - val_loss: 0.1928 - val_accuracy: 0.9343\n",
      "Epoch 29/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2223 - accuracy: 0.9210 - val_loss: 0.1978 - val_accuracy: 0.9307\n",
      "Epoch 30/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2162 - accuracy: 0.9231 - val_loss: 0.1959 - val_accuracy: 0.9265\n",
      "Epoch 31/50\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 0.2187 - accuracy: 0.9214 - val_loss: 0.1909 - val_accuracy: 0.9362\n",
      "Epoch 32/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2172 - accuracy: 0.9228 - val_loss: 0.2014 - val_accuracy: 0.9210\n",
      "Epoch 33/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2110 - accuracy: 0.9226 - val_loss: 0.1988 - val_accuracy: 0.9284\n",
      "Epoch 34/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2160 - accuracy: 0.9235 - val_loss: 0.1953 - val_accuracy: 0.9261\n",
      "Epoch 35/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2124 - accuracy: 0.9225 - val_loss: 0.1951 - val_accuracy: 0.9293\n",
      "Epoch 36/50\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 0.2216 - accuracy: 0.9238 - val_loss: 0.1932 - val_accuracy: 0.9256\n",
      "Epoch 37/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2099 - accuracy: 0.9254 - val_loss: 0.1981 - val_accuracy: 0.9265\n",
      "Epoch 38/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2149 - accuracy: 0.9203 - val_loss: 0.1966 - val_accuracy: 0.9320\n",
      "Epoch 39/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2116 - accuracy: 0.9195 - val_loss: 0.1916 - val_accuracy: 0.9288\n",
      "Epoch 40/50\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 0.2115 - accuracy: 0.9201 - val_loss: 0.1923 - val_accuracy: 0.9348\n",
      "Epoch 41/50\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 0.2099 - accuracy: 0.9227 - val_loss: 0.1950 - val_accuracy: 0.9376\n",
      "Epoch 42/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2074 - accuracy: 0.9246 - val_loss: 0.1930 - val_accuracy: 0.9353\n",
      "Epoch 43/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2116 - accuracy: 0.9243 - val_loss: 0.1969 - val_accuracy: 0.9247\n",
      "Epoch 44/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2160 - accuracy: 0.9234 - val_loss: 0.2055 - val_accuracy: 0.9265\n",
      "Epoch 45/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2107 - accuracy: 0.9230 - val_loss: 0.1983 - val_accuracy: 0.9265\n",
      "Epoch 46/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2050 - accuracy: 0.9268 - val_loss: 0.1933 - val_accuracy: 0.9366\n",
      "Epoch 47/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2124 - accuracy: 0.9219 - val_loss: 0.1939 - val_accuracy: 0.9330\n",
      "Epoch 48/50\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 0.2055 - accuracy: 0.9254 - val_loss: 0.1902 - val_accuracy: 0.9353\n",
      "Epoch 49/50\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 0.2067 - accuracy: 0.9264 - val_loss: 0.1961 - val_accuracy: 0.9275\n",
      "Epoch 50/50\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 0.2088 - accuracy: 0.9240 - val_loss: 0.1899 - val_accuracy: 0.9362\n"
     ]
    }
   ],
   "source": [
    "# Define the model-building function\n",
    "def build_model(hp):\n",
    "    model_NN = Sequential()\n",
    "    # Input layer and first hidden layer\n",
    "    model_NN.add(Dense(\n",
    "        units=hp.Int('units_layer1', min_value=32, max_value=256, step=32),\n",
    "        activation='relu',\n",
    "        input_shape=(16,)\n",
    "    ))\n",
    "    model_NN.add(Dropout(hp.Float('dropout_layer1', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "\n",
    "    # Second hidden layer\n",
    "    model_NN.add(Dense(\n",
    "        units=hp.Int('units_layer2', min_value=32, max_value=128, step=32),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model_NN.add(Dropout(hp.Float('dropout_layer2', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "\n",
    "    # Output layer\n",
    "    model_NN.add(Dense(len(np.unique(y)), activation='softmax'))\n",
    "\n",
    "    # Compile the model_NN\n",
    "    model_NN.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model_NN\n",
    "\n",
    "# Define the tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,  # Number of models to try\n",
    "    directory='my_dir',\n",
    "    project_name='tune_tabular_model'\n",
    ")\n",
    "\n",
    "# Run the tuner\n",
    "tuner.search(X_train, y_train, validation_split=0.2, epochs=50, batch_size=32)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"Best Hyperparameters: {best_hps.values}\")\n",
    "\n",
    "# Train the best model\n",
    "best_model_NN = tuner.hypermodel.build(best_hps)\n",
    "history = best_model_NN.fit(X_train, y_train, validation_split=0.2, epochs=50, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.93      0.89      0.91       265\n",
      "      BOMBAY       1.00      1.00      1.00       104\n",
      "        CALI       0.92      0.94      0.93       326\n",
      "    DERMASON       0.91      0.91      0.91       709\n",
      "       HOROZ       0.95      0.96      0.96       386\n",
      "       SEKER       0.92      0.96      0.94       406\n",
      "        SIRA       0.87      0.84      0.86       527\n",
      "\n",
      "    accuracy                           0.92      2723\n",
      "   macro avg       0.93      0.93      0.93      2723\n",
      "weighted avg       0.92      0.92      0.92      2723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predictions\n",
    "y_proba_NN = best_model_NN.predict(X_test)\n",
    "y_pred_NN = np.argmax(y_proba_NN, axis=1)\n",
    "print(classification_report(y_test, y_pred_NN, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.94      0.89      0.92       265\n",
      "      BOMBAY       1.00      1.00      1.00       104\n",
      "        CALI       0.94      0.94      0.94       326\n",
      "    DERMASON       0.90      0.92      0.91       709\n",
      "       HOROZ       0.97      0.95      0.96       386\n",
      "       SEKER       0.94      0.96      0.95       406\n",
      "        SIRA       0.86      0.86      0.86       527\n",
      "\n",
      "    accuracy                           0.92      2723\n",
      "   macro avg       0.94      0.93      0.93      2723\n",
      "weighted avg       0.92      0.92      0.92      2723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_RF = RandomForestClassifier(\n",
    "random_state=42)\n",
    "model_RF.fit(X_train, y_train)\n",
    "y_pred_RF = model_RF.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_RF)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_RF, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.94      0.90      0.92       265\n",
      "      BOMBAY       1.00      1.00      1.00       104\n",
      "        CALI       0.94      0.94      0.94       326\n",
      "    DERMASON       0.90      0.93      0.92       709\n",
      "       HOROZ       0.96      0.96      0.96       386\n",
      "       SEKER       0.96      0.96      0.96       406\n",
      "        SIRA       0.88      0.87      0.87       527\n",
      "\n",
      "    accuracy                           0.93      2723\n",
      "   macro avg       0.94      0.94      0.94      2723\n",
      "weighted avg       0.93      0.93      0.93      2723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create and train the XGBoost model\n",
    "model_XGB = XGBClassifier(\n",
    "    objective='multi:softmax',  # for multiclass classification\n",
    "    num_class=len(np.unique(y))  # number of classes\n",
    ")\n",
    "model_XGB.fit(X_train, y_train)\n",
    "# Step 3: Make predictions on the test set\n",
    "y_pred_XGB = model_XGB.predict(X_test)\n",
    "# Step 4: Evaluate the model_XGB\n",
    "accuracy = accuracy_score(y_test, y_pred_XGB)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "# Print detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_XGB, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.96      0.86      0.91       265\n",
      "      BOMBAY       1.00      1.00      1.00       104\n",
      "        CALI       0.92      0.94      0.93       326\n",
      "    DERMASON       0.92      0.91      0.92       709\n",
      "       HOROZ       0.97      0.95      0.96       386\n",
      "       SEKER       0.94      0.95      0.94       406\n",
      "        SIRA       0.85      0.89      0.87       527\n",
      "\n",
      "    accuracy                           0.92      2723\n",
      "   macro avg       0.94      0.93      0.93      2723\n",
      "weighted avg       0.92      0.92      0.92      2723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_SVM = SVC(\n",
    "kernel='rbf',\n",
    "C=1.0,\n",
    "gamma='scale',\n",
    "probability=True,\n",
    "random_state=42)\n",
    "model_SVM.fit(X_train, y_train)\n",
    "y_pred_SVM = model_SVM.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_SVM)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_SVM, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forming Enesemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple majority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Accuracy: 0.9243\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.94      0.90      0.92       265\n",
      "      BOMBAY       1.00      1.00      1.00       104\n",
      "        CALI       0.93      0.94      0.94       326\n",
      "    DERMASON       0.91      0.93      0.92       709\n",
      "       HOROZ       0.96      0.96      0.96       386\n",
      "       SEKER       0.94      0.96      0.95       406\n",
      "        SIRA       0.88      0.85      0.87       527\n",
      "\n",
      "    accuracy                           0.92      2723\n",
      "   macro avg       0.94      0.93      0.94      2723\n",
      "weighted avg       0.92      0.92      0.92      2723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine predictions using majority voting\n",
    "ensemble_predictions = mode([y_pred_NN, y_pred_RF, y_pred_XGB, y_pred_SVM], axis=0)[0].ravel()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, ensemble_predictions)\n",
    "print(f\"Ensemble Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, ensemble_predictions, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using combined probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 0s 3ms/step\n",
      "Ensemble Model Accuracy: 0.9251\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.94      0.90      0.92       265\n",
      "      BOMBAY       1.00      1.00      1.00       104\n",
      "        CALI       0.94      0.94      0.94       326\n",
      "    DERMASON       0.91      0.92      0.92       709\n",
      "       HOROZ       0.96      0.96      0.96       386\n",
      "       SEKER       0.93      0.96      0.95       406\n",
      "        SIRA       0.87      0.87      0.87       527\n",
      "\n",
      "    accuracy                           0.93      2723\n",
      "   macro avg       0.94      0.94      0.94      2723\n",
      "weighted avg       0.93      0.93      0.92      2723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_proba_NN = best_model_NN.predict(X_test)\n",
    "y_proba_RF = model_RF.predict_proba(X_test)\n",
    "y_proba_XGB = model_XGB.predict_proba(X_test)\n",
    "y_proba_SVM = model_SVM.predict_proba(X_test)\n",
    "\n",
    "# Combine probabilities by averaging\n",
    "ensemble_probs = (y_proba_NN * y_proba_RF * y_proba_XGB * y_proba_SVM) / 4\n",
    "\n",
    "# Get the class with the highest average probability\n",
    "ensemble_predictions = np.argmax(ensemble_probs, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, ensemble_predictions)\n",
    "print(f\"Ensemble Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, ensemble_predictions, target_names=label_encoder.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
